{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oig.hhs.gov/reports-and-publications/recommendations/tracker/?page=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xlsxwriter\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "class ReportScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.report_name = []\n",
    "        self.hhs_agencies = []\n",
    "        self.issued_dates = []\n",
    "        self.report_dict = {}\n",
    "        self.driver = None\n",
    "        self.report_links = []\n",
    "        self.CR = []\n",
    "        self.RIB=[]\n",
    "        self.pdf_path = None\n",
    "\n",
    "\n",
    "    def initialize_driver(self):\n",
    "        options = Options()\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    def is_url_reachable(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.status_code == 200\n",
    "        except requests.exceptions.RequestException:\n",
    "            return False\n",
    "        \n",
    "\n",
    "    def move_to_legacy(self, old_path,old_date, folder_type, pdf_filename, is_complete_report):\n",
    "        \n",
    "        if folder_type=='complete_reports':\n",
    "            shutil.move(old_path, legacy_cr)\n",
    "        else:\n",
    "            shutil.move(old_path, legacy_rib)\n",
    "        \n",
    "\n",
    "      \n",
    "\n",
    "    def extract_and_download_pdf(self, link,report, pdf_filename,date, pdf_path, is_complete_report):\n",
    "        folder_type = 'complete_reports' if is_complete_report else 'report_in_brief'\n",
    "        excel_filename=\"report_page_1.xlsx\"\n",
    "        if os.path.isfile(excel_filename):\n",
    "            data=pd.read_excel(excel_filename)\n",
    "            location = data.index[data.eq(report).any(axis=1)].tolist()\n",
    "            if not os.path.exists(pdf_path):  # Check if the file exists\n",
    "                self.download_pdf(link, pdf_path, pdf_filename, is_complete_report)\n",
    "            else:\n",
    "                for i in location:\n",
    "                    if data[\"Issued Date\"][i]!=date:\n",
    "                        if is_complete_report:\n",
    "                            self.move_to_legacy(data['complete report'][i],[\"Issued Date\"][i], folder_type, pdf_filename, is_complete_report)\n",
    "                            data['complete report'][i]=pdf_path\n",
    "                            self.download_pdf(link, pdf_path, pdf_filename,is_complete_report)\n",
    "                        else:\n",
    "                            self.move_to_legacy(data['report in brief'][i],[\"Issued Date\"][i], folder_type, pdf_filename, is_complete_report)\n",
    "                            data['report in brief'][i]=pdf_path\n",
    "                            self.download_pdf(link, pdf_path, pdf_filename,is_complete_report)\n",
    "                        data[\"Issued Date\"][i]==date\n",
    "        else:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                self.download_pdf(link, pdf_path, pdf_filename, is_complete_report)\n",
    "                        \n",
    "\n",
    "    def download_pdf(self, url, pdf_path, filename, is_complete_report):\n",
    "        response = requests.get(url)\n",
    "        with open(pdf_path, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "\n",
    "         \n",
    "        \n",
    "    def scrape_report(self, report_links,issued_dates,report_name):\n",
    "        if len(report_links)==len(issued_dates):\n",
    "            for i in range(len(report_links)):\n",
    "                    report = report_links[i]\n",
    "                    date=issued_dates[i]\n",
    "                    reportname=report_name[i]\n",
    "                    if report!= 'N/A':\n",
    "                        if report.endswith(\".asp\"):\n",
    "                            self.extract_pdf_links(report,date,reportname)\n",
    "                        else:\n",
    "                            if report.startswith(\"oig.\"):\n",
    "                                report_url = \"http://oig.hhs.gov\" + report\n",
    "                                if self.is_url_reachable(report_url):\n",
    "                                    pdf_filename = os.path.basename(report_url)\n",
    "                                    pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                                    path=os.path.abspath(pdf_path)\n",
    "                                    self.CR.append(os.path.abspath(pdf_path))\n",
    "                                    self.extract_and_download_pdf(report_url,reportname, pdf_filename,date, path, is_complete_report=True)\n",
    "                                    self.RIB.append('N/A')\n",
    "                                else:\n",
    "                                    self.CR.append('N/A')\n",
    "                                    self.RIB.append('N/A')\n",
    "                            else:\n",
    "                                if self.is_url_reachable(report):\n",
    "                                    pdf_filename = os.path.basename(report)\n",
    "                                    pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                                    path=os.path.abspath(pdf_path)\n",
    "                                    self.CR.append(os.path.abspath(pdf_path))\n",
    "                                    self.extract_and_download_pdf(report,reportname, pdf_filename,date, path, is_complete_report=True)\n",
    "                                    self.RIB.append('N/A')\n",
    "\n",
    "                                elif report.startswith(\"http://oig.hh.\"):\n",
    "                                    report=report[:13]+\"s\"+report[13:]\n",
    "                                    report=report[:4]+\"s\"+report[4:] \n",
    "                                    pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                                    path=os.path.abspath(pdf_path)\n",
    "                                    self.CR.append(os.path.abspath(pdf_path))\n",
    "                                    self.extract_and_download_pdf(report,reportname,pdf_filename,date, path, is_complete_report=True)\n",
    "                                    self.RIB.append('N/A')\n",
    "\n",
    "                                elif report.startswith(\"http:/\"):\n",
    "                                    report=report[:4]+\"s\"+report[4:]\n",
    "                                    pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                                    path=os.path.abspath(pdf_path)\n",
    "                                    self.CR.append(os.path.abspath(pdf_path))\n",
    "                                    self.extract_and_download_pdf(report,reportname, pdf_filename,date,path,  is_complete_report=True)\n",
    "                                    self.RIB.append('N/A')\n",
    "                                else:\n",
    "                                    self.CR.append('N/A')\n",
    "                                    self.RIB.append('N/A')\n",
    "                    else:\n",
    "                        self.CR.append('Report pending')\n",
    "                        self.RIB.append('Report pending')\n",
    "                \n",
    "\n",
    "    def extract_pdf_links(self, report_url,date,reportname):\n",
    "        self.driver.get(report_url)\n",
    "        time.sleep(5)  # Wait for the page to load (adjust as needed)\n",
    "        if self.is_url_reachable(report_url):\n",
    "            soup1 = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            a_elements = soup1.find('p', class_='report-metadata').find_all('a')\n",
    "            href_links = [a['href'] for a in a_elements]\n",
    "            if len(href_links) == 2:\n",
    "                for i in href_links:\n",
    "                    link = \"https://oig.hhs.gov\" + i\n",
    "                    if link.endswith(\"RIB.pdf\"):\n",
    "                        if self.is_url_reachable(link):\n",
    "                            pdf_filename = os.path.basename(link)\n",
    "                            pdf_path = os.path.join(current_rib, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.RIB.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(link, reportname,pdf_filename,date,path,  is_complete_report=False)\n",
    "                        \n",
    "                        elif report.startswith(\"http://oig.hh.\"):\n",
    "                            report=report[:13]+\"s\"+report[13:]\n",
    "                            report=report[:4]+\"s\"+report[4:] \n",
    "                            pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.RIB.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(report,reportname,pdf_filename,date, path, is_complete_report=False)\n",
    "                            \n",
    "                        elif report.startswith(\"http:/\"):\n",
    "                            report=report[:4]+\"s\"+report[4:]\n",
    "                            pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.RIB.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(report,reportname, pdf_filename,date, path,is_complete_report=False)\n",
    "                            \n",
    "                        else:\n",
    "                            self.RIB.append('N/A')\n",
    "                    else:\n",
    "                        if self.is_url_reachable(link):\n",
    "                            pdf_filename = os.path.basename(link)\n",
    "                            pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.CR.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(link,reportname, pdf_filename,date,path, is_complete_report=True)\n",
    "                        \n",
    "                        elif report.startswith(\"http://oig.hh.\"):\n",
    "                            report=report[:13]+\"s\"+report[13:]\n",
    "                            report=report[:4]+\"s\"+report[4:] \n",
    "                            pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.CR.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(report,reportname, pdf_filename,date, path,is_complete_report=True)  \n",
    "\n",
    "                        elif report.startswith(\"http:/\"):\n",
    "                            report=report[:4]+\"s\"+report[4:]\n",
    "                            pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.CR.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(report, reportname,pdf_filename, date,path,is_complete_report=True)\n",
    "                            \n",
    "                        else:\n",
    "                            self.CR.append('N/A')\n",
    "            else:\n",
    "                for i in href_links:\n",
    "                    link = \"https://oig.hhs.gov\" + i\n",
    "                    if self.is_url_reachable(link):\n",
    "                        pdf_filename = os.path.basename(link)\n",
    "                        pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                        path=os.path.abspath(pdf_path)\n",
    "                        self.CR.append(os.path.abspath(pdf_path))\n",
    "                        self.extract_and_download_pdf(link,reportname, pdf_filename,date,path,is_complete_report=True)\n",
    "                        self.RIB.append(\"N/A\")\n",
    "\n",
    "                    elif report.startswith(\"http://oig.hh.\"):\n",
    "                            report=report[:13]+\"s\"+report[13:]\n",
    "                            report=report[:4]+\"s\"+report[4:] \n",
    "                            pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                            path=os.path.abspath(pdf_path)\n",
    "                            self.CR.append(os.path.abspath(pdf_path))\n",
    "                            self.extract_and_download_pdf(report,reportname, pdf_filename,date,path, is_complete_report=True)\n",
    "                            self.RIB.append('N/A')\n",
    "\n",
    "                    elif report.startswith(\"http:/\"):\n",
    "                        report=report[:4]+\"s\"+report[4:]\n",
    "                        pdf_path = os.path.join(current_cr, pdf_filename)\n",
    "                        path=os.path.abspath(pdf_path)\n",
    "                        self.CR.append(os.path.abspath(pdf_path))\n",
    "                        self.extract_and_download_pdf(report,reportname, pdf_filename,date, path, is_complete_report=True)\n",
    "                        self.RIB.append('N/A')\n",
    "                    else:\n",
    "                        self.CR.append('N/A')\n",
    "                        self.RIB.append(\"N/A\")\n",
    "        else:\n",
    "            self.CR.append(\"N/A\")\n",
    "            self.RIB.append(\"N/A\")\n",
    "\n",
    "    def create_report_dict(self, report_name, hhs_agencies, issued_dates, CR, RIB):\n",
    "        \n",
    "        for i in range(len(report_name)):\n",
    "            self.report_dict[report_name[i]] = {\n",
    "                \"HHS Agency\": hhs_agencies[i],\n",
    "                \"Issued Date\": issued_dates[i],\n",
    "                \"complete report\": CR[i],\n",
    "                \"report in brief\":RIB[i]\n",
    "            }\n",
    "        return self.report_dict\n",
    "\n",
    "\n",
    "    def create_metadata_file(self, excel_filename,report_dict):\n",
    "        if not os.path.isfile(excel_filename):\n",
    "            report_df = pd.DataFrame.from_dict(self.report_dict, orient='index')\n",
    "            report_df= report_df.reset_index()\n",
    "            report_df = report_df.rename(columns={'index': 'Report Name'})\n",
    "            with pd.ExcelWriter(excel_filename, engine='xlsxwriter') as writer:\n",
    "                report_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "        else:\n",
    "            os.remove(excel_filename)\n",
    "            report_df = pd.DataFrame.from_dict(self.report_dict, orient='index')\n",
    "            report_df= report_df.reset_index()\n",
    "            report_df = report_df.rename(columns={'index': 'Report Name'})\n",
    "            with pd.ExcelWriter(excel_filename, engine='xlsxwriter') as writer:\n",
    "                report_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "            \n",
    "\n",
    "    def run_scraper(self, num_pages, excel_filename):\n",
    "        \n",
    "        self.initialize_driver()\n",
    "\n",
    "        for page_number in range(1, num_pages + 1):\n",
    "            page_url = self.base_url + str(page_number)\n",
    "            print(page_url)\n",
    "            self.driver.get(page_url)\n",
    "            time.sleep(5)\n",
    "            soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            report_elements_xpath = \"/html//section[@id='results']/div[@class='grid-container']/div[2]/div[@class='grid-col-fill']/ul[1]/li/div[@class='usa-card__container']\"\n",
    "            report_elements = self.driver.find_elements(By.XPATH, report_elements_xpath)\n",
    "\n",
    "            for report_element in report_elements:\n",
    "                title_element = report_element.find_element(By.XPATH, \".//header[@class='usa-card__header']/h2\")\n",
    "                self.report_name.append(title_element.text.strip())\n",
    "\n",
    "            dl_elements = soup.find_all('dl', class_='pep-metadata--inline')\n",
    "            for ele in dl_elements:\n",
    "                dt_elements = ele.find_all('dt', class_='pep-metadata__term')\n",
    "                dd_elements = ele.find_all('dd', class_='pep-metadata__def')\n",
    "                elements = ele.find_all('a', class_='usa-link')\n",
    "\n",
    "                hhs_agency = dd_elements[1].text.strip() if len(dd_elements) > 1 else 'N/A'\n",
    "                issued_date = dd_elements[2].text.strip() if len(dd_elements) > 2 else 'N/A'\n",
    "\n",
    "                self.hhs_agencies.append(hhs_agency)\n",
    "                self.issued_dates.append(issued_date)\n",
    "\n",
    "            dl_elements = soup.find_all('dl', class_='pep-metadata--inline')\n",
    "            for ele in dl_elements:\n",
    "                elements = ele.find_all('a', class_='usa-link')\n",
    "                report_link = elements[0]['href'].strip() if elements else 'N/A'\n",
    "                self.report_links.append(report_link)\n",
    "\n",
    "            self.scrape_report(self.report_links,self.issued_dates,self.report_name)\n",
    "\n",
    "            self.report_links = []\n",
    "\n",
    "        report_dict=self.create_report_dict(self.report_name, self.hhs_agencies, self.issued_dates, self.CR, self.RIB)\n",
    "        \n",
    "        self.create_metadata_file(excel_file,report_dict)\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_cr = 'current/complete_reports'\n",
    "current_rib='current/report_in_brief'\n",
    "legacy_cr='legacy/complete_reports'\n",
    "legacy_rib='legacy/report_in_brief'\n",
    "if not os.path.exists(current_cr):\n",
    "    os.makedirs(current_cr)\n",
    "if not os.path.exists(current_rib):\n",
    "    os.makedirs(current_rib)\n",
    "if not os.path.exists(legacy_cr):\n",
    "    os.makedirs(legacy_cr)\n",
    "if not os.path.exists(legacy_rib):\n",
    "    os.makedirs(legacy_rib)\n",
    "\n",
    "\n",
    "base_url = \"https://oig.hhs.gov/reports-and-publications/recommendations/tracker/?page=\"\n",
    "\n",
    "excel_file = 'report_page_1.xlsx'\n",
    "                \n",
    "scraper = ReportScraper(base_url)\n",
    "\n",
    "# Run scraper\n",
    "scraper.run_scraper(num_pages=1,excel_filename=excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.8 kB ? eta -:--:--\n",
      "   ------- ------------------------------- 30.7/154.8 kB 640.0 kB/s eta 0:00:01\n",
      "   -------------------- ------------------ 81.9/154.8 kB 919.0 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 122.9/154.8 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 154.8/154.8 kB 768.7 kB/s eta 0:00:00\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\rhithika sree\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rhithika sree\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\rhithika sree\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rhithika sree\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip  install  xlsxwriter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
